<!DOCTYPE html>
<html>
<head>
<title>WeCoS17 Abstracts</title>
<meta name="generator" content="Bluefish 2.2.8" >
<meta name="author" content="Vito DP Servedio" >
<meta name="date" content="2017-09-18T18:08:38+0200" >
<meta name="description" content="abstracts">
<meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta http-equiv="content-type" content="application/xhtml+xml; charset=UTF-8">
<meta http-equiv="content-style-type" content="text/css">
<meta http-equiv="expires" content="0">
<link href="speakers.css" rel="stylesheet" type="text/css">
</head>
<body>

<h1 id="Staab">Steffen Staab</h1>
<h2>Web Science: Objectives and Some Methods</h2>

<p>
Web Science is about understanding the evolution of the Web and related socio-technical systems in a way that helps its further beneficial development.
Given that both human activity and technical constraints co-constitute the Web, Web Science is necessarily a heavily interdisciplinary endeavor exploiting
a diverse set of  methods ranging from ethnographics to engineering. In this talk I will give some examples how complexity science contributes to understanding
the Web, hopefully leading to discussions about how complexity science may contribute to the advancement of Web Science in the future.
</p>

<hr>

<h1 id="Holyst">Janusz Holyst</h1>
<h2>Efficient detection of spread source in large complex networks</h2>

<p>
Spread over complex networks is a ubiquitous process with increasingly wide applications.  
Locating spread sources is often important, e.g. finding the patient one in epidemics, or 
source of rumor spreading in social network.  Pinto, Thiran and Vetterli introduced an 
algorithm (PTVA) to solve the important case of this problem in which a limited set of nodes 
act as observers and report times at which the spread reached them.  
PTVA uses all observers to find a solution.  Here we propose a new approach in
 which observers with low quality information (i.e. with large spread encounter times) 
 are ignored and potential sources are selected based on the likelihood gradient from high quality observers. 
The original complexity of PTVA is $O(N^\alpha)$, where $\alpha \in (3,4)$ depends 
on the network topology and number of observers. 
Our Gradient Maximum Likelihood Algorithm (GMLA) reduces this complexity to 
$O(N^2\log(N))$.  Extensive numerical tests performed on synthetic networks and 
real Facebook network demonstrate that for scale-free networks GMLA yields higher 
quality localization results than PTVA does.  Counterintuitively, increasing the number 
of nearest observing nodes over some critical value may decrease the quality of 
localization results in GMLA.
</p>

<hr>

<h1 id="Mitrovic">Marija Mitrovic</h1>
<h2>Collective social phenomena on the Web: a statistical physics perspective</h2>

<p>
The abundance of large-scale data resulting from online interactions has enabled the 
study of various collective social phenomena on the Web. Methods and tools from 
statistical physics and complex network theory have proven to be a useful tool for 
quantitative description and uncovering the mechanisms that may underly the collective
 behaviour on the Web. We have applied these techniques for characterisation and a better
  understanding of two fundamental collective social phenomena on the Web: collective 
  emotions [1,2,3] and collective knowledge building [4]. 
  Complex network theory provides us with quantitative details about the structure and 
  evolution of social network between Web users. Time series analysis and entropy measures 
  allow us to study in details dynamics of interacting online social systems. With agent based
   modelling we can explore the connection between the structure of online social
    phenomena and dynamics, and determine the important system properties and
     mechanism that are responsible for the emergence of collective social phenomena. 
     Here we will demonstrate how these techniques are used for the quantitative study 
     of the collective emotional behaviour of Web users by applying them to empirically 
     investigate the data from different online social sites, including Blogs, Digg and MySpace.
      In this context, the mesoscopic structure of social networks reflected in distinct user 
      communities and avalanches of affect-bearing messages appear as the signature of 
      collective emotional behaviour [1,2]. We use the agent-based model of emotional bloggers 
      to study the emergence and robustness of these collective states [3].
</p>

<ul>
<li>
			[1] Mitrović M, Paltoglou G and Tadić B, Quantitative analysis of bloggers’ collective behavior 
			powered by emotions, JSTAT 2011, P02005 (2011).</li>
<li>
			[2] Mitrović M, Paltoglou G and Tadić B., Networks and emotion-driven user communities at 
			popular blogs, Eur. Phys. J. B 77, 597-609 (2010).</li>
<li>
			[3] Mitrović M and Tadić B., Dynamics of bloggers communities: bipartite networks 
			from empirical data and agent-based modeling, Physica A 391, 5264-5278 (2012).</li>
<li>
			[4] Mitrović Dankulov M, Melnik R and Tadić B., The dynamics of meaningful social 
			interactions and the emergence of collective knowledge, Sci. Rep. 5, 12197 (2015).</li>
</ul>

<hr>

<h1 id="Sienkiewicz">Julian Sienkiewicz</h1>
<h2>Impact and modeling of lexical and sentiment factors on the popularity of scientific papers</h2>

<p>
We investigate how textual properties of scientific papers relate to the number of citations they receive. 
Our main finding is that correlations are nonlinear and affect differently the most cited and typical papers. 
For instance, we find that, in most journals, short titles correlate positively with citations only for the most 
cited papers, whereas for typical papers, the correlation is usually negative. Our analysis of six different factors, 
calculated both at the title and abstract level of 4.3 million papers in over 1500 journals in the Web of Science 
database, reveals the number of authors, and the length and complexity of the abstract, as having the strongest (positive) 
influence on the number of citations [1]. During the second part of my talk, using PLOS database and associated 
ALM (Article Level Metrics) I shall also consider how reader's specific actions (HTML view, PDF download and citations) 
correlate with indicators of textual properties at different structural levels (title, abstract, full text). A simple stochastic 
model of transitions between these levels will be introduced [2].
</p>

<ul>
<li> [1] J. Sienkiewicz, E. G. Altmann, R. Soc. Open Sci. 3, 160140 (2016).</li>
<li> [2] J. Sienkiewicz, E. G. Altmann, unpublished.</li>
</ul>

<hr>

<h1 id="Hotho">Andreas Hotho</h1>
<h2>Learning Semantic Relatedness From Human Feedback Using Metric Learning</h2>

<p>
Assessing the degree of semantic relatedness between words is an important task 
with a variety of semantic applications, such as ontology learning for the Semantic Web, 
semantic search or query expansion. To accomplish this in an automated fashion, many 
relatedness measures have been proposed. However, most of these metrics only encode 
information contained in the underlying corpus and thus do not directly model human intuition. 
To solve this, we propose to utilize a metric learning approach to improve existing semantic 
relatedness measures by learning from additional information, such as explicit human feedback. 
For this, we argue to use word embeddings instead of traditional high-dimensional vector 
representations in order to leverage their semantic density and to reduce computational cost. 
We rigorously test our approach on several domains including tagging data as well as publicly 
available embeddings based on Wikipedia texts and navigation. Human feedback about semantic 
relatedness for learning and evaluation is extracted from publicly available datasets such as MEN or WS-353. 
We find that our method can significantly improve semantic relatedness measures by learning from additional information, 
such as explicit human feedback. For tagging data, we are the first to generate and study embeddings. 
Our results are of special interest for ontology and recommendation engineers, but also for any other researchers
 and practitioners of Semantic Web techniques. 
 <a href="https://arxiv.org/abs/1705.07425">arxiv.org/abs/1705.07425</a>
</p>

<hr>

<h1 id="Tadic">Bosiljka Tadić</h1>
<h2>Modeling Online Human Behavior and Emergent Structure of Social Networks</h2>

<p>
In recent years, extensive records of human online interactions provide
 opportunities to analyze social dynamics with the accuracy analogous
  to the study of driven dynamical systems in the physics laboratory. 
  It has been recognized that, despite different purpose and the action 
  rules at various Web sites, the online social systems exhibit certain 
  universal features, which then can be utilized to model human online 
  behavior [1] and predict the evolution of the underlying social networks [2]. 
  Recently, we have developed agent-based modeling approach where the 
  agents' features are designed to be statistically similar to the profiles of 
  users inferred from the corresponding empirical data in blogs [3,4], chats [5], 
  online social network MySpace [6- 9], and Questions & Answers [10-12]. 
  In all studied cases, the “human factor” manifests in (i) the universal distribution 
  of the activity profiles, (ii) the system-characteristic interactivity times, and 
  (iii) the appearance of a relatively stable social graph, even in the absence of any in 
  advance connections. Moreover, all systems vitally depend on off-line events and 
  the pace of the new users' arrival. In contrast to the physical systems in the laboratory, 
  the human online interactions strongly depend on the communicated contents 
  (cognitive or emotional), which can be inferred from the text of the transmitted 
  messages by machine learning techniques. In the model, these features of the 
  agents are modeled by the dynamical variables which influence the course of the 
  agent's action and, at the same time, get changed under the impact of others and 
  the external input. In our approaches, the simulated sequence of events is then analyzed 
  as time series containing the specified contents and mapped onto networks. Using graph 
  theory and algebraic topology of graphs we determine the structural characteristics of these 
  networks and relate them to the model parameters and the agent's features. 
  In this presentation, our focus is knowledge creation by social communications with Questions & Answers. 
  We review the emergence of higher combinatorial topology in the knowledge networks and 
  demonstrate how the physical principle of self-organized criticality underlies the creation of 
  collective knowledge.
</p>
<ul>
<li> [1] B. Tadić, Modeling behavior of Web users as agents with reason and sentiment, in Advances in Computational Modeling Research: Theory, Developments and Applications, A.B. Cora, ed., Novapublishing, New York (2013)</li><li> [2] B. Tadić, V. Gligorijević, M. Mitrović, M. Šuvakov, Co-Evolutionary Mechanisms of Emotional Bursts in Online Social Dynamics and Networks, Entropy 15, 5084-5120 (2013)</li><li> [3] M. Mitrović, G. Paltoglou, B. Tadić, Networks and emotion-driven user communities at popular blogs, The European Physical Jourbal B 77, 597-609 (2010)</li><li> [4] M. Mitrović, B. Tadić, Dynamics of bloggers' communities: Bipartite networks from empirical data and agent-based modeling, Physica A: Statistical Mechanics and its Applications 391(21), 5264-5278 (2012) </li>
<li> [5] V. Gligorijević, M. Skowron, B. Tadić, Structure and stability of of online chat networks built on emotion-carrying links, Physica A: Statistical Mechanics and its Applications 392 (3), 538-543 (2013) </li>
<li> [6] M. Šuvakov, M. Mitrović, V. Gligorijević, B. Tadić, How the online social networks are used: Dialogues- based structure of MySpace, J. Royal Society Interface 10, 20120819 (2012)</li><li> [7] M. Andjelković, B. Tadić, S. Maletić, M. Rajković, Hierarchical sequencing of online social graphs, Physica A: Statistical Mechanics and its Applications, 436, 582-595 (2015)</li><li> [8] B. Tadić, M. Šuvakov, D. Garcia, F. Schweitzer, Agent-Based Simulations of Emotional Dialogs in the Online Social Network MySpace, in Cyberemotions, pp. 207-229, Springer International Publishing, (2017) </li>
<li> [9] B. Tadić, The dynamical structure of MySpace, in The SAGE Encyclopedia of Internet (to appear)<li> [10] M. Mitrović Dankulov, R. Melnik, B. Tadić, The dynamics of meaningful social interactions and the emergence of collective knowledge, Scientific Reports 5, 12197 (2015)</li><li> [11] M. Andjelković, B.Tadić, M.Mitrović Dankulov, M. Rajković, R. Melnik, Topology of Innovation Spaces in the Knowledge Networks emerging through Questions-and-Answers, PLOS One 11, e0154655 (2016) </li>
<li> [12] B.Tadić, M.Mitrović Dankulov, R. Melnik, The mechanisms of self-organized criticality in social processes of knowledge creation, Physical Review E, (2017) in press.</li>
</ul>
<hr>

<h1 id="Fernandez">Javier Fernandez</h1>
<h2>Democratizing Big Semantic Data management</h2>

<p>
"Linked Open Data" is a collective effort for the integration and 
combination of data from diverse sources, converting existing 
scattered data in the Web into profitable knowledge. 
Data providers make use of a common graph-based model, 
the Resource Description Framework (RDF), to describe and link data with 
various degrees of structure (or lack thereof). The potential of this Big Semantic
 Data is under-exploited when data management is based on traditional, 
 human-readable RDF representations, which add unnecessary overheads when storing, 
 exchanging and consuming RDF in the context of a large-scale and machine-understandable 
 Semantic Web. In this talk we will first discuss the main challenges emerging in a Big Semantic 
 Data scenario, and we will present fundamental concepts of Compact Data Structures and RDF 
 self-indexes. Then, we will introduce HDT, a compact data structure and binary serialization 
 format that keeps big RDF datasets compressed while maintaining search and browse operations 
 without prior decompression. Finally, we will show the application of HDT to represent and query 
 more than 28 billion edges of the current Linked Open Data network. 
</p>
<hr>

<h1 id="Tessone">Claudio Tessone</h1>
<h2>Technocracy and fixed Incentive schemes: the centralised doom for Bitcoin</h2>

<p>
Nowadays, most facets of human behaviour are pervaded by technical systems that 
facilitate our information and economic exchanges. In recent years, aiming at more 
resilient and scalable systems, many have been redesigned as decentralised ones. 
Blockchain has disrupted thinking about distributed systems: this mechanism allows 
information to be diffused securely across a network without requiring a central (trusted) 
authority to guarantee its truthfulness. The digital currency Bitcoin provides a prime example: 
it is implemented on top of a blockchain, and its value is solely assigned by a (largely speculative) 
market setup upon its inception with a predefined incentive scheme by a small group of 
developers. 
Nowadays, Bitcoin can be seen as a closed economy: having followed a technocratic approach 
in its immutable design, it is the only case of an economy where all monetary transactions can 
be traced back with full detail. In this contribution, we show its fixed incentive scheme has led to 
high levels of centralisation and economic flow, in stark contrast to its original conception.
</p>
<hr>

<h1 id="Burdick">Mark Burdick</h1>
<h2>Social Network and Topic Modeling Analysis of US Political Blogosphere</h2>

<p>
The presentation at WeCoS will primarily focus on preliminary findings from an 
experimental dataset of blog posts collected during the 2016 US presidential 
election. This data was collected daily from ten popular political blogs over 
the course of one month. The analysis shows similarly politically oriented blogs are
 more likely to link to each other than adversarial blogs. In addition, after applying 
 community analysis, two main groups of blogs were identified. In applying topic 
 modeling to the blog posts of these two groups, the groups are shown to focus on 
 different events, terms, and stories in their coverage of the 2016 presidential campaign. 
 Toward the larger goal of understanding and visualizing how political rhetoric is 
 distributed in an online social network and how network structure modulates its 
 propagation, these result show that longitudinal social network analysis and topic 
 modeling are promising methodologies. The data collection currently underway is 
 an expansion of this methodology to 40 US political blogs over the course of nine 
 months. From these analyses, agent-based models, predictive models and interactive 
 visualizations will be made for displaying mechanisms of topic propagation. 
 This project will contribute to the growing body of research on the role of social media 
 in political communication, in particular, the dynamics of rhetoric in a blogosphere.
</p>
<hr>


<h1 id="Savenkov">Vadim Savenkov</h1>
<h2>A case of schema mappings evolution: characterization via limits</h2>

<p>
The notion of limit of sequences of structures has been recently studied in the 
context of large networks with the motivation of replacing a large graph (like the 
link graph of the Internet), with its samples. Whereas the large graph as a whole 
can be impossible to describe at each point of time, one can draw samples of predefined 
size from it. As the underlying graph evolves, a sequence of samples can be obtained, 
and the task of characterizing the properties that persist in this sequence arises.
<br>
In this talk we consider sequences of objects of a similar sort, namely sequences 
of mappings between structures also known as schema mappings. Schema mappings 
are often defined as binary relations between instances of data schemata, which in 
basic cases can be expressed in a declarative way. A prominent application of mappings 
on the Web is ontology mediated data access. For instance, the core of DBpedia is a semantic 
graph extracted from Wikipedia using a set of declarative mapping assertions,  developed 
and maintained by the user community.
<br>
We take an abstract viewpoint on the dynamics of mappings and consider a very 
special case where mapping evolution can be described as refinement. 
Mathematically, a mapping under continuous refinement can be represented 
as a sequence of snapshots in which the distance between versions decreases
 with time. In such a setting, can one define a limit object to which the sequence 
 of snapshots converges? What are the properties of such a limit, in particular given 
 that the mappings are specified in any of the well understood mapping languages?
</p>
<ul>
<li> [1] Ph.Kolaitis, R.Pichler, E.Sallinger, V.Savenkov: Limits of Schema Mappings, Proc. of ICDT 2016, LIPIcs (48), Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik 2016, <a href="http://drops.dagstuhl.de/opus/volltexte/2016/5788/pdf/18.pdf">http://drops.dagstuhl.de/opus/volltexte/2016/5788/pdf/18.pdf</a></li><li> [2] L. Lovasz: Large networks and Graph Limits, Colloquium Publications 60, AMS 2012 (Preprint: <a href="http://web.cs.elte.hu/~lovasz/bookxx/hombook-almost.final.pdf">http://web.cs.elte.hu/~lovasz/bookxx/hombook-almost.final.pdf</a>)</li><li> [3] J. Nesetril, P. Ossona de Mendez: Sparsity – Graphs, Structures, and Algorithms, volume 28 of Algorithms and combinatorics. Springer, 2012. </li></ul>
<hr>

<a href="index.html">back to main page</a>
</body>
</html>
